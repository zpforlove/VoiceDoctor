import io
import wave
import numpy as np
import torch
import streamlit as st
import tempfile
import os
import re
import av
import time
from typing import Optional, List, Dict

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    GenerationConfig
)

from streamlit_webrtc import (
    webrtc_streamer,
    WebRtcMode,
    RTCConfiguration,
    AudioProcessorBase
)

from funasr import AutoModel
from MeloTTS.melo.api import TTS
from av.audio.resampler import AudioResampler

# ======================= å£°çº¹è¯†åˆ«å¯¼å…¥ =======================
import torchaudio
from speechbrain.inference.classifiers import EncoderClassifier

# ======================= DeepSeek API å·¥å…· =======================
from openai import OpenAI

# ======================= WebRTC VAD =======================
import webrtcvad


@st.cache_resource
def init_deepseek_client() -> Optional["OpenAI"]:
    """åˆå§‹åŒ– DeepSeek API å·¥å…·ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å– DEEPSEEK_API_KEYï¼‰"""
    if OpenAI is None:
        st.warning("æœªæ£€æµ‹åˆ° openai åº“ï¼Œè¯·å…ˆæ‰§è¡Œï¼špip install openai")
        return None
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        st.warning("æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼Œæ— æ³•è°ƒç”¨ DeepSeekã€‚")
        return None
    try:
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        return client
    except Exception as e:
        st.error(f"åˆå§‹åŒ– DeepSeek API å·¥å…·å¤±è´¥ï¼š{e}")
        return None


def _build_emr_prompt(raw_dialogue: str) -> List[dict]:
    """
    æ„é€  DeepSeek æç¤ºè¯ï¼š
    1. å°† ASR åŸå§‹å¯¹è¯è§„èŒƒåŒ–ä¸ºå®Œæ•´çš„ä¹¦é¢è½¬å†™ã€‚
    2. åŸºäºè§„èŒƒåŒ–å†…å®¹ï¼Œæå–å¹¶ç”Ÿæˆä¸­æ–‡ç”µå­ç—…å†ï¼ˆEMRï¼‰ã€‚
    """
    system_msg = (
        "ä½ æ˜¯ä¸€åä¸´åºŠæ–‡ä¹¦ä¸ä¸­æ–‡ç”µå­ç—…å†ï¼ˆEMRï¼‰è§„èŒƒåŒ–ä¸“å®¶ã€‚\n"
        "è¯·åŸºäºæä¾›çš„å£è¿°å°±è¯Šå¯¹è¯ï¼ˆå«ASRè½¬å†™ï¼‰ï¼Œä¸¥æ ¼æŒ‰é¡ºåºå®Œæˆä»¥ä¸‹ä¸¤é¡¹ä»»åŠ¡ï¼š\n\n"
        "**ä»»åŠ¡ä¸€ï¼šå¯¹è¯è§„èŒƒåŒ–**\n"
        "1.  **çº æ­£**ï¼šè¯†åˆ«å¹¶çº æ­£ ASR è¯¯è¯†åˆ«ã€å£è¯­åŒ–è¡¨è¿°ã€ä»¥åŠä¸å½“çš„æ–­å¥ã€‚\n"
        "2.  **é‡è¿°**ï¼šä½¿ç”¨ä¸“ä¸šçš„åŒ»å­¦æœ¯è¯­ä¸æ ‡å‡†å•ä½é‡è¿°ï¼ˆä¾‹å¦‚ï¼šæ—¥æœŸ/æ—¶é—´ã€è¯ç‰©å‰‚é‡ã€èº«ä½“éƒ¨ä½ç­‰ï¼‰ã€‚\n"
        "3.  **è¾“å‡º**ï¼šåœ¨ç¬¬ä¸€ä¸ªæ ‡é¢˜ä¸‹ï¼Œè¾“å‡ºç»è¿‡ä¸Šè¿°å¤„ç†åçš„**å®Œæ•´å¯¹è¯å…¨æ–‡**ã€‚\n\n"
        "4.  **ä¿ç•™è§’è‰²**ï¼š**å¿…é¡»** ä¸¥æ ¼ä¿ç•™åŸå§‹å¯¹è¯ä¸­çš„è¯´è¯äººæ ‡æ³¨ï¼ˆä¾‹å¦‚ 'User A:', 'User B:'ï¼‰ï¼Œ**ç¦æ­¢** å°†å…¶æ›¿æ¢ä¸ºä»»ä½•å…¶ä»–è§’è‰²ã€‚\n"
        "**ä»»åŠ¡äºŒï¼šEMRç»“æ„åŒ–æå–**\n"
        "1.  **æå–**ï¼šåŸºäº**è§„èŒƒåŒ–å**çš„å¯¹è¯å†…å®¹ï¼Œæå–ä¿¡æ¯å¹¶é€é¡¹å¡«å†™ç—…å†ã€‚\n"
        "2.  **çº¦æŸ**ï¼š\n"
        "    - ä¸¥æ ¼ä¿ç•™äº‹å®ä¸æ—¶åºï¼Œä¸è¿›è¡Œä¸»è§‚æ¨æ–­æˆ–è‡†é€ ä¿¡æ¯ã€‚\n"
        "    - å¯¹äºå¯¹è¯ä¸­ç¡®å®**æœªæåŠ**çš„ä¿¡æ¯ï¼Œåœ¨ç›¸åº”é¡¹ç›®ä¸­æ˜ç¡®å†™å…¥â€œ**æœªæåŠ**â€ã€‚\n\n"
        "**è¾“å‡ºæ ¼å¼**\n"
        "è¯·ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ä¸­æ–‡ Markdown æ ¼å¼ï¼Œ**å¿…é¡»å…ˆè¾“å‡ºâ€œè§„èŒƒåŒ–ASRè½¬å†™å…¨æ–‡â€ï¼Œå†è¾“å‡ºåç»­ç—…å†æ¸…å•**ï¼š\n\n"
        "## è§„èŒƒåŒ–ASRè½¬å†™å…¨æ–‡\n"
        "...\n\n"
        "## ä¸»è¯‰\n"
        "...\n\n"
        "## ç°ç—…å²\n"
        "...\n\n"
        "## æ—¢å¾€å²\n"
        "...\n\n"
        "## è¿‡æ•å²\n"
        "...\n\n"
        "## ç”¨è¯å²\n"
        "...\n\n"
        "## å®¶æ—å²\n"
        "...\n\n"
        "## åˆæ­¥å°è±¡ / é‰´åˆ«è¯Šæ–­\n"
        "...\n\n"
        "## å»ºè®®ä¸è®¡åˆ’\n"
        "..."
    )
    user_msg = (
        "ä»¥ä¸‹æ˜¯é€å¥ ASR è½¬å†™çš„å°±è¯Šå£è¿°ï¼ˆåŒ…å«è¯´è¯äººæ ‡æ³¨ï¼‰ã€‚"
        "è¯·æ®æ­¤ç”Ÿæˆ**è§„èŒƒåŒ–å…¨æ–‡**å’Œ**ç»“æ„åŒ–ä¸­æ–‡ EMR**ï¼š\n\n"
        f"{raw_dialogue}"
    )
    return [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ]


def call_deepseek_emr(client: "OpenAI", raw_dialogue: str) -> str:
    """
    è°ƒç”¨ DeepSeek å¯¹ ASR æ‹¼æ¥ç»“æœè¿›è¡Œ EMR è§„èŒƒåŒ–ã€‚å¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    try:
        messages = _build_emr_prompt(raw_dialogue)
        resp = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            stream=False,
            temperature=0.2,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        st.error(f"DeepSeek è°ƒç”¨å¤±è´¥ï¼š{e}")
        return ""


# ======================= é¡µé¢ä¸å…¨å±€é…ç½® =======================
st.set_page_config(page_title="MedLLM åŒ»ç–—è¯­éŸ³åŠ©æ‰‹", page_icon="ğŸ©º", layout="centered")
st.title("ğŸ©º MedLLM åŒ»ç–—è¯­éŸ³åŠ©æ‰‹")


def do_rerun():
    """å…¼å®¹ä¸åŒç‰ˆæœ¬ streamlit çš„é‡è·‘"""
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()


# ======================= TTS =======================
def Text_to_audio(Text: str) -> Optional[bytes]:
    """
    è¿”å›åˆæˆå¥½çš„ WAV å­—èŠ‚ï¼Œå¹¶ç”¨äºæŒä¹…åŒ–åˆ°å¯¹è¯å†å²ä¸­ã€‚
    """
    speed = 1.0
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    try:
        model = TTS(language='ZH', device=device)
        speaker_ids = model.hps.data.spk2id
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            out_path = tmp.name
        model.tts_to_file(Text, speaker_ids['ZH'], out_path, speed=speed)
        with open(out_path, "rb") as f:
            data = f.read()
        try:
            os.unlink(out_path)
        except Exception:
            pass
        return data
    except Exception as e:
        st.warning(f"TTS å¤±è´¥ï¼š{e}")
        return None


# ======================= SenseVoice ASR =======================
@st.cache_resource
def load_sensevoice_model():
    return AutoModel(
        model="iic/SenseVoiceSmall",
        device="cuda:0" if torch.cuda.is_available() else "cpu",
        disable_update=True,
        disable_download=False
    )


def asr_from_wav(wav_bytes: bytes) -> str:
    model = load_sensevoice_model()
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
        tmp_file.write(wav_bytes)
        tmp_file_path = tmp_file.name

    try:
        result = model.generate(input=tmp_file_path, batch_size_s=0, language="auto")
        if result and len(result) > 0:
            raw_text = result[0].get("text", "")
            # æ¸…ç† <|...|> ç­‰ç‰¹æ®Šæ ‡è®°
            clean_text = re.sub(r'<\|[^>]+\|>', '', raw_text)
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            print(f"[DEBUG] ASR åŸå§‹: {raw_text}")
            print(f"[DEBUG] ASR æ¸…ç†: {clean_text}")
            return clean_text
        return ""
    except Exception as e:
        print(f"[ASR] é”™è¯¯: {e}")
        return ""


# ======================= MedLLM =======================
@st.cache_resource
def init_medllm():
    model_path = "/mnt/data/VoiceDoctor/Baichuan2-7B-MedLLM-Merged"
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=quantization_config,
    )
    try:
        gen_cfg = GenerationConfig.from_pretrained(model_path)
        model.generation_config = gen_cfg
    except Exception:
        pass
    try:
        model.generation_config.use_cache = False
    except Exception:
        pass

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True,
    )
    return model, tokenizer


def medllm_chat(model, tokenizer, messages: List[dict]) -> str:
    if hasattr(model, "chat"):
        return model.chat(tokenizer, messages, stream=False)

    def _join_msgs(msgs):
        segs = []
        for m in msgs:
            role = m.get("role", "user")
            content = m.get("content", "")
            if role == "user":
                segs.append(f"[ç”¨æˆ·]\n{content}")
            else:
                segs.append(f"[åŠ©æ‰‹]\n{content}")
        segs.append("[åŠ©æ‰‹]\n")
        return "\n".join(segs)

    prompt = _join_msgs(messages)
    inputs = tokenizer(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
        model = model.to("cuda")
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=tokenizer.eos_token_id,
        )
    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    last = text.rsplit("[åŠ©æ‰‹]", 1)[-1].strip()
    return last


# ======================= å£°çº¹è¯†åˆ« (SPKREC) =======================

@st.cache_resource
def load_speaker_model():
    """åŠ è½½ SpeechBrain ECAPA-TDNN å£°çº¹è¯†åˆ«æ¨¡å‹"""
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    try:
        # ä½¿ç”¨ /tmp ç›®å½•ä¸‹çš„å›ºå®šè·¯å¾„æ¥ç¼“å­˜æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡é‡å¯éƒ½ä¸‹è½½
        save_dir = os.path.join("/tmp", "speechbrain_models", "spkrec-ecapa-voxceleb")
        os.makedirs(save_dir, exist_ok=True)
        model = EncoderClassifier.from_hparams(
            source="speechbrain/spkrec-ecapa-voxceleb",
            savedir=save_dir,
            run_opts={"device": device}
        )
        model.eval()
        print(f"å£°çº¹è¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œäº {device}")
        return model
    except Exception as e:
        st.error(f"åŠ è½½å£°çº¹æ¨¡å‹å¤±è´¥: {e}")
        return None


def _load_wav_to_tensor(wav_bytes: bytes) -> Optional[torch.Tensor]:
    """ä» WAV å­—èŠ‚åŠ è½½ 16k å•å£°é“å¼ é‡"""
    try:
        with io.BytesIO(wav_bytes) as f:
            wav, sr = torchaudio.load(f)

        # VAD å¤„ç†å™¨å·²ç»ä¿è¯äº† 16kï¼Œä½†ä»¥é˜²ä¸‡ä¸€
        if sr != 16000:
            wav = torchaudio.functional.resample(wav, sr, 16000)

        # ç¡®ä¿å•å£°é“
        if wav.ndim > 1 and wav.shape[0] > 1:
            wav = torch.mean(wav, dim=0, keepdim=True)

        return wav
    except Exception as e:
        print(f"[SPKREC] ä»å­—èŠ‚åŠ è½½ WAV å¤±è´¥: {e}")
        return None


# ======================= recognize_speaker =======================
def recognize_speaker(wav_bytes: bytes, spk_model: EncoderClassifier) -> str:
    """
    è¯†åˆ«è¯´è¯äººï¼Œå¦‚æœæ˜¯æ–°è¯´è¯äººï¼Œåˆ™åˆ†é…ä¸€ä¸ªæ–° ID (User A, B, C...)
    å¦‚æœåŒ¹é…åˆ°å·²æœ‰è¯´è¯äººï¼Œåˆ™ä½¿ç”¨ EMA æ›´æ–°è¯¥è¯´è¯äººçš„å¹³å‡ embeddingã€‚
    """
    if spk_model is None:
        return "User"  # Fallback

    signal = _load_wav_to_tensor(wav_bytes)
    if signal is None:
        return "User"  # Fallback

    # æ£€æŸ¥éŸ³é¢‘æ—¶é•¿ï¼Œå¦‚æœå¤ªçŸ­ï¼ˆ<0.5sï¼‰ï¼Œembedding è´¨é‡æå·®ï¼Œè·³è¿‡è¯†åˆ«
    dur_s = signal.shape[1] / 16000.0
    if dur_s < 0.5:
        print(f"[SPKREC] éŸ³é¢‘ç‰‡æ®µè¿‡çŸ­ ({dur_s:.2f}s)ï¼Œè·³è¿‡å£°çº¹è¯†åˆ«ã€‚")
        return "User"  # è¿”å›ä¸€ä¸ªé€šç”¨ ID

    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    signal = signal.to(device)

    try:
        with torch.no_grad():
            # æå– embedding
            new_embedding = spk_model.encode_batch(signal)
            # (1, 1, 192) -> (192)
            new_embedding = new_embedding.squeeze().cpu()

            # ç»Ÿä¸€ L2 å½’ä¸€åŒ– (æ–°åµŒå…¥)
            new_embedding = torch.nn.functional.normalize(new_embedding, p=2, dim=0)

    except Exception as e:
        print(f"[SPKREC] æå– embedding å¤±è´¥: {e}")
        return "User"

    ss = st.session_state

    if not ss.known_speakers:
        # è¿™æ˜¯ç¬¬ä¸€ä¸ªè¯´è¯äºº
        new_name = "User A"
        ss.known_speakers.append({
            "name": new_name,
            "embedding": new_embedding,  # å•ä½å‘é‡
            "num_samples": 1
        })
        print(f"[SPKREC] æ–°å¢ç¬¬ä¸€ä¸ªè¯´è¯äºº: {new_name}")
        return new_name

    similarity = torch.nn.CosineSimilarity(dim=0, eps=1e-6)

    # --- æœç´¢æœ€ä½³åŒ¹é… ---
    max_sim = -1.0
    best_match_speaker = None

    for speaker in ss.known_speakers:
        known_embedding = speaker["embedding"]
        sim = similarity(new_embedding, known_embedding).item()

        if sim > max_sim:
            max_sim = sim
            best_match_speaker = speaker

    # --- è¯´è¯äººç›¸ä¼¼åº¦é˜ˆå€¼ ---
    threshold = ss.get("spk_threshold", 0.3)

    if best_match_speaker and max_sim >= threshold:
        # --- åŒ¹é…æˆåŠŸï¼šæ›´æ–°å¹³å‡ Embedding ---
        matched_name = best_match_speaker["name"]
        print(f"[SPKREC] åŒ¹é…åˆ°: {matched_name} (ç›¸ä¼¼åº¦: {max_sim:.2f})")

        try:
            old_emb = best_match_speaker["embedding"]
            n = best_match_speaker["num_samples"]

            # --- EMA æ›´æ–°åœ¨å•ä½çƒé¢ä¸Š ---
            ema_alpha = 0.15
            updated_emb = (1 - ema_alpha) * old_emb + ema_alpha * new_embedding
            updated_emb = torch.nn.functional.normalize(updated_emb, p=2, dim=0)

            best_match_speaker["embedding"] = updated_emb
            best_match_speaker["num_samples"] = n + 1
            print(f"[SPKREC] æ›´æ–° {matched_name} çš„å¹³å‡ embedding (æ ·æœ¬æ•°: {n + 1})")
        except Exception as e:
            print(f"[SPKREC] æ›´æ–° embedding å¤±è´¥: {e}")

        return matched_name

    else:
        # --- åŒ¹é…å¤±è´¥ï¼šæ³¨å†Œæ–°ç”¨æˆ· ---
        next_id = len(ss.known_speakers)
        if next_id < 26:
            new_name = f"User {chr(ord('A') + next_id)}"
        else:
            new_name = f"User {chr(ord('A') + (next_id % 26))}{next_id // 26}"

        ss.known_speakers.append({
            "name": new_name,
            "embedding": new_embedding,
            "num_samples": 1
        })
        print(f"[SPKREC] æ–°å¢è¯´è¯äºº: {new_name} (æœ€é«˜ç›¸ä¼¼åº¦: {max_sim:.2f}, ä½äºé˜ˆå€¼ {threshold})")
        return new_name


# ======================= WebRTC éŸ³é¢‘å¤„ç†å™¨ï¼ˆwebrtcvad åˆ†å¥ï¼‰ =======================
class BrowserAudioProcessor(AudioProcessorBase):
    """
    - ç»Ÿä¸€é‡é‡‡æ ·ä¸º 16k/mono/s16
    - ä½¿ç”¨ WebRTC VADï¼ˆpy-webrtcvadï¼‰è¿›è¡Œå¸§çº§è¯­éŸ³æ´»åŠ¨æ£€æµ‹
    - é™éŸ³ç´¯è®¡ >= 0.5s è§†ä¸ºä¸€å¥ç»“æŸï¼Œæ¨å…¥ ready_segmentsï¼ˆWAV bytesï¼‰
    - prepad 120ms è¿›å…¥ä¸€å¥æ—¶è¡¥ä¸Šå‰å¯¼ï¼Œtail ä¿ç•™ 500ms
    - ä¸Šå±‚ UI ä¼šé€æ¡å±•ç¤ºæ¯ä¸ªç‰‡æ®µï¼ˆæ–‡æœ¬ + éŸ³é¢‘ï¼‰
    """

    def __init__(self):
        # è¾“å‡ºé‡‡æ ·è®¾ä¸º 16kï¼Œä»¥æ»¡è¶³ webrtcvad æ”¯æŒçš„é‡‡æ ·ç‡
        self.sr_out = 16000
        self.resampler = AudioResampler(format="s16", layout="mono", rate=self.sr_out)

        # WebRTC VADï¼š0(å®½æ¾)~3(æ¿€è¿›)ï¼Œè¿™é‡Œå– 2ï¼Œå®æµ‹å£è¯­å¯¹è¯æ¯”è¾ƒç¨³å¥
        self.vad = webrtcvad.Vad(2)

        self.is_collecting = False
        self.input_rate = None

        # webrtcvad ä»…æ”¯æŒ 10/20/30ms å¸§ï¼›ä¿æŒ 20ms
        self.frame_ms = 20
        self.frame_len = int(self.sr_out * self.frame_ms / 1000)

        # å¥å­è¾¹ç•Œ & è¿‡æ»¤
        self.silence_limit_s = 0.5
        self.min_utt_s = 0.35

        # è¿›å…¥ä¸€å¥æ—¶çš„å‰ç½®ç¼“å†² & å¥æœ«ä¿ç•™
        self.prepad_ms = 120
        self.prepad_len = int(self.sr_out * self.prepad_ms / 1000)
        self.tail_keep_ms = 500
        self.tail_keep_len = int(self.sr_out * self.tail_keep_ms / 1000)

        # ç¼“å­˜ä¸çŠ¶æ€
        self._carry = np.zeros(0, dtype=np.int16)
        self._pre_buffer = np.zeros(0, dtype=np.int16)
        self._utter_active = False
        self._utter_pcm = np.zeros(0, dtype=np.int16)
        self._silence_accum_s = 0.0

        self._ready_segments: List[bytes] = []

    def reset(self):
        self._carry = np.zeros(0, dtype=np.int16)
        self._pre_buffer = np.zeros(0, dtype=np.int16)
        self._utter_active = False
        self._utter_pcm = np.zeros(0, dtype=np.int16)
        self._silence_accum_s = 0.0

    def start_collect(self):
        self.reset()
        self.is_collecting = True
        print("[DEBUG] å¼€å§‹é‡‡é›†ï¼ˆç­‰å¾…è¯´è¯è§¦å‘é¦–å¥ï¼‰...")

    def stop_collect(self):
        self.is_collecting = False
        print("[DEBUG] åœæ­¢é‡‡é›†ã€‚è¾“å…¥é‡‡æ ·ç‡:", self.input_rate)

    def _export_wav_bytes(self, pcm: np.ndarray) -> bytes:
        buf = io.BytesIO()
        with wave.open(buf, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(self.sr_out)
            wf.writeframes(pcm.astype(np.int16, copy=False).tobytes())
        return buf.getvalue()

    def _vad_is_speech(self, frame_i16: np.ndarray) -> bool:
        """
        ä½¿ç”¨ webrtcvad åˆ¤æ–­å•å¸§æ˜¯å¦ä¸ºè¯­éŸ³ã€‚
        ä¼ å…¥å‚æ•°å¿…é¡»æ˜¯ 16k/å•å£°é“/16-bit PCMï¼Œé•¿åº¦=10/20/30msã€‚
        """
        try:
            # å¿…é¡»æ˜¯ bytesï¼ˆlittle-endian 16-bit PCMï¼‰
            return self.vad.is_speech(frame_i16.tobytes(), sample_rate=self.sr_out)
        except Exception as e:
            # æ•è·å¼‚å¸¸é¿å…ä¸­æ–­éŸ³é¢‘ç®¡çº¿
            print(f"[VAD] is_speech å¼‚å¸¸: {e}")
            return False

    def _finalize_utter_if_needed(self, force: bool = False):
        if self._utter_active and (force or self._silence_accum_s >= self.silence_limit_s):
            pcm = self._utter_pcm
            if self.tail_keep_len > 0 and self._carry.size > 0:
                tail = self._carry[:self.tail_keep_len]
                pcm = np.concatenate([pcm, tail], axis=0)

            dur = pcm.size / float(self.sr_out)
            if dur >= self.min_utt_s:
                wav_bytes = self._export_wav_bytes(pcm)
                self._ready_segments.append(wav_bytes)
                print(f"[VAD] å¥å­å®Œæˆ: {dur:.2f}s, é˜Ÿåˆ—æ•°={len(self._ready_segments)}")
            else:
                print(f"[VAD] ä¸¢å¼ƒè¿‡çŸ­ç‰‡æ®µ: {dur:.2f}s")

            self._utter_active = False
            self._utter_pcm = np.zeros(0, dtype=np.int16)
            self._silence_accum_s = 0.0
            self._pre_buffer = np.zeros(0, dtype=np.int16)

    def force_flush_current_utterance(self):
        self._finalize_utter_if_needed(force=True)

    def pop_ready_segment_wav(self) -> Optional[bytes]:
        if self._ready_segments:
            return self._ready_segments.pop(0)
        return None

    def _frame_to_int16_mono_16k(self, frame: av.AudioFrame) -> List[np.ndarray]:
        out = []
        if self.input_rate is None:
            try:
                fmt = frame.format.name if frame.format else "unknown"
                layout = frame.layout.name if frame.layout else "unknown"
            except Exception:
                fmt, layout = "unknown", "unknown"
            self.input_rate = getattr(frame, "sample_rate", None)
            print(f"[DEBUG] æ£€æµ‹åˆ°è¾“å…¥é‡‡æ ·ç‡: {self.input_rate}, æ ¼å¼: {fmt}, å¸ƒå±€: {layout}")

        try:
            out_frames = self.resampler.resample(frame) or []
        except Exception as e:
            print(f"[ERROR] é‡é‡‡æ ·å¤±è´¥: {e}")
            return out

        for f in out_frames:
            arr = f.to_ndarray()
            if arr.ndim == 2:
                arr = arr[0]
            arr = np.asarray(arr, dtype=np.int16).reshape(-1)
            out.append(arr)
        return out

    def _process_pcm_for_vad(self, pcm: np.ndarray):
        cat = pcm if self._carry.size == 0 else np.concatenate([self._carry, pcm], axis=0)
        n = (cat.size // self.frame_len) * self.frame_len
        frames = cat[:n].reshape(-1, self.frame_len)
        self._carry = cat[n:]

        for fr in frames:
            is_speech = self._vad_is_speech(fr)

            if not self._utter_active:
                # ç»´æŠ¤è¿›å…¥ä¸€å¥çš„å‰ç½®ç¼“å†²ï¼ˆæœ€å¤š prepad_lenï¼‰
                if self._pre_buffer.size == 0:
                    self._pre_buffer = fr.copy()
                else:
                    self._pre_buffer = np.concatenate([self._pre_buffer, fr], axis=0)[-self.prepad_len:]

                if is_speech and self.is_collecting:
                    self._utter_active = True
                    # å¸¦ä¸Š prepad æå‰é‡ï¼Œé¿å…é¦–éŸ³çˆ†ç ´è¢«åˆ‡æ‰
                    if self._pre_buffer.size > 0:
                        self._utter_pcm = self._pre_buffer.copy()
                    else:
                        self._utter_pcm = np.zeros(0, dtype=np.int16)
                    self._utter_pcm = np.concatenate([self._utter_pcm, fr], axis=0)
                    self._silence_accum_s = 0.0
            else:
                self._utter_pcm = np.concatenate([self._utter_pcm, fr], axis=0)
                if is_speech:
                    self._silence_accum_s = 0.0
                else:
                    self._silence_accum_s += self.frame_ms / 1000.0
                    if self._silence_accum_s >= self.silence_limit_s:
                        self._finalize_utter_if_needed(force=False)

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        try:
            chunks = self._frame_to_int16_mono_16k(frame)
            if self.is_collecting and chunks:
                for ch in chunks:
                    self._process_pcm_for_vad(ch)
        except Exception as e:
            print(f"[ERROR] å¤„ç†éŸ³é¢‘å¸§å¤±è´¥: {e}")
        return frame


# ======================= ä¼šè¯çŠ¶æ€ =======================
def ensure_state():
    ss = st.session_state
    ss.setdefault("messages", [])  # æ¯æ¡å¯åŒ…å« {"role","content","audio"(bytes|None)}
    ss.setdefault("rec_active", False)
    # asr_segments: List[Dict]ï¼Œæ¯æ¡åŒ…å« {"text": str, "wav": bytes, "speaker": str}
    ss.setdefault("asr_segments", [])
    ss.setdefault("need_loop", False)  # æ§åˆ¶è½®è¯¢

    # --- å£°çº¹è¯†åˆ«çŠ¶æ€ ---
    ss.setdefault("known_speakers", [])

    # --- é˜ˆå€¼ ---
    ss.setdefault("spk_threshold", 0.3)

    # --- ä¿å­˜æœ€è¿‘ä¸€æ¬¡ DeepSeek è§„èŒƒåŒ–ç—…å† ---
    ss.setdefault("last_emr_text", "")


ensure_state()


def _append_message(role: str, content: str, audio: Optional[bytes] = None):
    """
    ç»Ÿä¸€å…¥å£ï¼šä»…å†™å…¥ stateï¼Œä¸åšå³æ—¶ UI æ¸²æŸ“ï¼›å¹¶åšç®€å•å»é‡ï¼Œé¿å…é‡å¤ appendã€‚
    """
    msgs = st.session_state["messages"]
    if msgs:
        last = msgs[-1]
        if last.get("role") == role and (last.get("content") or "") == (content or ""):
            return
    st.session_state["messages"].append({"role": role, "content": content, "audio": audio})


# ======================= å¯åŠ¨å³åˆå§‹åŒ– ASRã€LLMã€SPKRECã€DeepSeek =======================
with st.spinner("åŠ è½½ ASR æ¨¡å‹ä¸­â€¦"):
    _ = load_sensevoice_model()
with st.spinner("åŠ è½½ MedLLM æ¨¡å‹ä¸­â€¦"):
    model, tokenizer = init_medllm()
with st.spinner("åŠ è½½å£°çº¹è¯†åˆ«æ¨¡å‹ä¸­â€¦"):
    speaker_model = load_speaker_model()
with st.spinner("åˆå§‹åŒ– DeepSeek API å·¥å…·â€¦"):
    ds_client = init_deepseek_client()

# ======================= æ»šåŠ¨å®¹å™¨ï¼šèŠå¤©å†å² =======================
st.subheader("ğŸ—¨ï¸ å¯¹è¯å†å²")
chat_box = st.container(height=520, border=True)
with chat_box:
    if not st.session_state["messages"]:
        with st.chat_message("assistant", avatar="ğŸ©º"):
            st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ MedLLM åŒ»ç–—åŠ©æ‰‹å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚")
    for m in st.session_state["messages"]:
        avatar = "ğŸ§‘â€ğŸ’»" if m["role"] == "user" else "ğŸ©º"
        with st.chat_message(m["role"], avatar=avatar):
            st.markdown(m.get("content", ""))
            if m.get("audio"):
                st.audio(m["audio"], format="audio/wav")

# ======================= å½•éŸ³ UI =======================
st.subheader("ğŸ¤ æµè§ˆå™¨å½•éŸ³ï¼ˆStart â†’ webrtcvad è‡ªåŠ¨åˆ†å¥ â†’ å®æ—¶è½¬å†™+éŸ³é¢‘é¢„è§ˆ â†’ DeepSeekä¸€é”®è½¬è¯‘ â†’ è¾“å…¥AIåŒ»ç”Ÿï¼‰")
st.caption(
    "æ“ä½œï¼šå…ˆç‚¹å‡»ä¸Šæ–¹å†…ç½® **Start** â†’ ç‚¹å‡» **å¼€å§‹å½•éŸ³** â†’ è®²è¯ï¼›ä½¿ç”¨ **WebRTC VAD** æ£€æµ‹ï¼Œé™éŸ³â‰¥0.5ç§’è‡ªåŠ¨åˆ‡å¥ï¼›"
    "æ¯å¥ä¼šæ˜¾ç¤º **[è¯´è¯äººID]** + è½¬å†™ + è¯¥æ®µéŸ³é¢‘ï¼›"
    "ç‚¹å‡» **DeepSeekä¸€é”®è½¬è¯‘** ç»“æŸå½•éŸ³å¹¶ç”Ÿæˆè§„èŒƒåŒ–ç—…å†ï¼›å¦‚éœ€ç»§ç»­é—®è¯Šï¼Œç‚¹å‡» **è¾“å…¥AIåŒ»ç”Ÿ**ã€‚"
)

rtc_config = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})
ctx = webrtc_streamer(
    key="speech",
    mode=WebRtcMode.SENDONLY,
    rtc_configuration=rtc_config,
    media_stream_constraints={
        # --- å¼€å¯æµè§ˆå™¨ç«¯ 3A å¤„ç† ---
        "audio": {"echoCancellation": True, "noiseSuppression": True, "autoGainControl": True},
        "video": False
    },
    audio_processor_factory=BrowserAudioProcessor,
    async_processing=False,
)

# ä¸‰åˆ—æŒ‰é’®ï¼šå¼€å§‹å½•éŸ³ã€DeepSeekä¸€é”®è½¬è¯‘ã€è¾“å…¥AIåŒ»ç”Ÿ
col_rec1, col_rec2, col_rec3 = st.columns(3)
with col_rec1:
    start_btn = st.button("å¼€å§‹å½•éŸ³ â–¶ï¸", type="primary", use_container_width=True)
with col_rec2:
    ds_btn = st.button("DeepSeekä¸€é”®è½¬è¯‘ ğŸ§ ", use_container_width=True)
with col_rec3:
    send_llm_btn = st.button("è¾“å…¥AIåŒ»ç”Ÿ ğŸ¤–", use_container_width=True)

# -------------- äº‹ä»¶ï¼šDeepSeekä¸€é”®è½¬è¯‘ï¼ˆç»“æŸå½•éŸ³å¹¶è§„èŒƒåŒ–ç—…å†ï¼‰ --------------
if ds_btn:
    if ctx and ctx.audio_processor and st.session_state["rec_active"]:
        # åœæ­¢é‡‡é›†å¹¶å¼ºåˆ¶å†²åˆ·å°¾å¥
        ctx.audio_processor.stop_collect()
        ctx.audio_processor.force_flush_current_utterance()
        st.session_state["need_loop"] = False

        # Drain æ‰€æœ‰å¾…å¤„ç†ç‰‡æ®µ
        popped_items: List[Dict] = []
        while True:
            seg = ctx.audio_processor.pop_ready_segment_wav()
            if seg is None:
                break
            # æ£€æŸ¥æ—¶é•¿
            try:
                with io.BytesIO(seg) as f, wave.open(f, 'rb') as wf:
                    frames = wf.getnframes()
                    rate = wf.getframerate()
                    dur_s = frames / float(rate)
                if dur_s < 1.0:
                    print(f"[FILTER] ä¸¢å¼ƒè¿‡çŸ­ç‰‡æ®µ: {dur_s:.2f}s (é˜ˆå€¼ 1.0s)")
                    continue
            except Exception as e:
                print(f"[FILTER] æ£€æŸ¥éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
                continue

            # å£°çº¹ + ASR
            speaker_id = recognize_speaker(seg, speaker_model)
            txt = asr_from_wav(seg)
            if txt:
                formatted_text = f"{speaker_id}: {txt}"  # å»æ‰ markdown ç²—ä½“ï¼Œæ›´åˆ©äºåç»­å¤„ç†
                item = {"text": formatted_text, "wav": seg, "speaker": speaker_id}
                popped_items.append(item)
            else:
                print(f"[FILTER] ä¸¢å¼ƒç©ºè½¬å†™ç»“æœ (æ¥è‡ª {speaker_id})")

        # æŠŠæœ€åå†²åˆ·å‡ºæ¥çš„ç‰‡æ®µæ›´æ–°åˆ° UI çŠ¶æ€ä¸­ï¼ˆç”¨äºâ€œå®æ—¶è½¬å†™â€å±•ç¤ºçš„ä¸€è‡´æ€§ï¼‰
        st.session_state["asr_segments"].extend(popped_items)

        # æ„é€ åŸå§‹å¯¹è¯ä¸²ï¼ˆé€è¡Œï¼‰
        raw_lines = [it.get("text", "") for it in st.session_state.get("asr_segments", []) if it.get("text")]
        raw_dialogue = "\n".join(raw_lines).strip()

        # é‡ç½®é‡‡é›†å™¨ä¸è½®è¯¢
        ctx.audio_processor.reset()
        st.session_state["rec_active"] = False
        if hasattr(ctx, "stop"):
            try:
                ctx.stop()
            except:
                pass
        elif hasattr(ctx, "request_stop"):
            try:
                ctx.request_stop()
            except:
                pass

        # æ¸…ç©ºä¸´æ—¶è½¬å†™ä¸å£°çº¹åº“ï¼ˆè¿™ä¸€è½®å·²å®Œæˆï¼‰
        st.session_state["asr_segments"] = []
        st.session_state["known_speakers"] = []

        if not raw_dialogue:
            st.info("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹ï¼Œæœ¬æ¬¡æœªå‘é€è‡³ DeepSeekã€‚")
        else:
            if ds_client is None:
                st.error("DeepSeek API å·¥å…·ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ openai å®‰è£…ä¸ DEEPSEEK_API_KEYã€‚")
            else:
                with st.spinner("DeepSeek æ­£åœ¨è§„èŒƒåŒ–ç—…å†â€¦"):
                    emr_text = call_deepseek_emr(ds_client, raw_dialogue)

                if emr_text:
                    # ä¿å­˜åˆ° session å¹¶è¾“å‡ºåˆ°å¯¹è¯å†å²ï¼ˆassistant èº«ä»½å±•ç¤ºâ€œè§„èŒƒåŒ–ç—…å†â€ï¼‰
                    st.session_state["last_emr_text"] = emr_text
                    _append_message(
                        "assistant",
                        "### ğŸ“‹ DeepSeek è§„èŒƒåŒ–ç—…å†\n\n" + emr_text
                    )
                    do_rerun()
                else:
                    st.error("æœªå¾—åˆ°æœ‰æ•ˆçš„è§„èŒƒåŒ–ç—…å†æ–‡æœ¬ã€‚")
    else:
        st.warning("å½“å‰æ²¡æœ‰æ­£åœ¨è¿›è¡Œçš„å½•éŸ³ã€‚è¯·å…ˆç‚¹å‡»ã€å¼€å§‹å½•éŸ³ã€ã€‚")

# -------------- äº‹ä»¶ï¼šè¾“å…¥AIåŒ»ç”Ÿï¼ˆæŠŠæœ€è¿‘ä¸€æ¬¡ DeepSeek è§„èŒƒåŒ–ç—…å†é€å…¥ MedLLM å¹¶ TTSï¼‰ --------------
if send_llm_btn:
    emr_text = st.session_state.get("last_emr_text", "").strip()
    if not emr_text:
        st.warning("æ²¡æœ‰å¯ç”¨çš„è§„èŒƒåŒ–ç—…å†æ–‡æœ¬ï¼Œè¯·å…ˆæ‰§è¡Œã€DeepSeekä¸€é”®è½¬è¯‘ã€ã€‚")
    else:
        # 1. æ„é€ ä¸€ä¸ªä¸´æ—¶çš„æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å½“å‰å†å²å’Œå³å°†å‘é€çš„ EMR æ–‡æœ¬
        messages_for_llm = st.session_state["messages"].copy()
        messages_for_llm.append({"role": "user", "content": emr_text})

        try:
            # 2. ä½¿ç”¨è¿™ä¸ªä¸´æ—¶åˆ—è¡¨æ¥è°ƒç”¨ LLM
            reply = medllm_chat(model, tokenizer, messages_for_llm)
        except Exception as e:
            reply = f"å¯¹è¯å¤±è´¥ï¼š{e}"

        # 3. LLM çš„å›å¤è¢«æ­£å¸¸è¿½åŠ åˆ° *æ°¸ä¹…* å¯¹è¯å†å²ä¸­
        audio_bytes = Text_to_audio(Text=reply)
        _append_message("assistant", reply, audio=audio_bytes)
        do_rerun()

# -------------- äº‹ä»¶ï¼šå¼€å§‹å½•éŸ³ --------------
if start_btn:
    if ctx and ctx.state.playing:
        if ctx.audio_processor:
            ctx.audio_processor.start_collect()
            st.session_state["rec_active"] = True
            # æ¸…ç©ºä¸Šä¸€è½®çŠ¶æ€
            st.session_state["asr_segments"] = []
            st.session_state["known_speakers"] = []
            st.session_state["need_loop"] = True
            st.info("æ­£åœ¨å½•éŸ³â€¦ï¼ˆWebRTC VADï¼šé™éŸ³â‰¥0.5ç§’è‡ªåŠ¨åˆ‡å¥ï¼›æ¯å¥é™„å¸¦éŸ³é¢‘é¢„è§ˆï¼‰")
        else:
            st.error("éŸ³é¢‘å¤„ç†å™¨æœªå°±ç»ªï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•ã€‚")
    else:
        st.warning("è¯·å…ˆç‚¹å‡»ä¸Šæ–¹å†…ç½®çš„ **Start** æŒ‰é’®ä»¥å»ºç«‹éº¦é£è¿æ¥ï¼ˆæµè§ˆå™¨ä¼šå¼¹å‡ºæƒé™è¯·æ±‚ï¼‰ã€‚")


# -------------- å®æ—¶ drainï¼šå¥æ®µè½¬å†™ï¼ˆå¸¦å£°çº¹ï¼‰ --------------
def _drain_ready_segments_into_state() -> int:
    """
    ä¿®æ”¹åçš„ç‰ˆæœ¬ï¼š
    - æ¯æ¬¡åªå¤„ç†é˜Ÿåˆ—ä¸­çš„ *ä¸€ä¸ª* ç‰‡æ®µï¼Œå¹¶ç«‹å³è¿”å›ã€‚
    - è¿”å›å€¼ï¼š1 (å¤„ç†æˆåŠŸ1ä¸ª), 0 (é˜Ÿåˆ—ä¸ºç©ºæˆ–å¤„ç†å¤±è´¥)
    """
    if not (ctx and ctx.audio_processor):
        return 0

    # 1. æ¯æ¬¡åªå°è¯• Pop ä¸€ä¸ªç‰‡æ®µ
    seg = ctx.audio_processor.pop_ready_segment_wav()
    if seg is None:
        # é˜Ÿåˆ—ä¸ºç©ºï¼Œè¿”å› 0
        return 0

    # 2. æ£€æŸ¥æ—¶é•¿
    try:
        with io.BytesIO(seg) as f, wave.open(f, 'rb') as wf:
            frames = wf.getnframes()
            rate = wf.getframerate()
            dur_s = frames / float(rate)
        if dur_s < 1.0:
            print(f"[FILTER] ä¸¢å¼ƒè¿‡çŸ­ç‰‡æ®µ: {dur_s:.2f}s (é˜ˆå€¼ 1.0s)")
            return 0  # ç®—ä½œå¤„ç†äº†ï¼Œä½†ä¸ç®—æœ‰æ•ˆç»“æœ
    except Exception as e:
        print(f"[FILTER] æ£€æŸ¥éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
        return 0

    # 3. å£°çº¹ + ASR
    speaker_id = recognize_speaker(seg, speaker_model)
    text = asr_from_wav(seg)

    if text:
        formatted_text = f"{speaker_id}: {text}"
        item = {"text": formatted_text, "wav": seg, "speaker": speaker_id}
        st.session_state["asr_segments"].append(item)
        print(f"[UI] å¤„ç†å®Œæˆ 1 ä¸ªå¥æ®µ: {formatted_text}")
        return 1  # æˆåŠŸå¤„ç† 1 ä¸ª
    else:
        print(f"[FILTER] ä¸¢å¼ƒç©ºè½¬å†™ç»“æœ (æ¥è‡ª {speaker_id})")
        return 0  # ç®—ä½œå¤„ç†äº†ï¼Œä½†ä¸ç®—æœ‰æ•ˆç»“æœ


# ======================= å®æ—¶è½¬å†™åŒºåŸŸ =======================
if len(st.session_state["asr_segments"]) > 0:
    st.subheader("ğŸ“ å®æ—¶è½¬å†™")
    asr_box = st.container(height=360, border=True)
    with asr_box:
        for item in st.session_state["asr_segments"]:
            with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
                st.markdown(item.get("text", ""))
                wav_bytes = item.get("wav", None)
                if wav_bytes:
                    st.audio(wav_bytes, format="audio/wav")

# -------------- è½®è¯¢åˆ·æ–°ï¼ˆä»…åœ¨å½•éŸ³ä¸­ï¼‰ --------------
if st.session_state["rec_active"] and st.session_state.get("need_loop", False):

    # 1. å°è¯•ä»éŸ³é¢‘å¤„ç†å™¨ä¸­è·å–å¹¶å¤„ç†ï¼ˆASR+SPKRECï¼‰ *ä¸€ä¸ª* å·²å°±ç»ªçš„ç‰‡æ®µ
    processed_count = _drain_ready_segments_into_state()

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç»“æœ
    if processed_count > 0:
        # 2a. å¦‚æœå¤„ç†å¾—åˆ°äº†1ä¸ªæ–°ç‰‡æ®µ (ASRæœ‰ç»“æœ)ï¼Œç«‹å³é‡ç»˜UIï¼Œä¸ç¡çœ 
        #     (do_rerun() ä¹‹åä¼šé©¬ä¸Šå†æ¬¡è¿›å…¥æ­¤å¾ªç¯ï¼Œç»§ç»­å¤„ç†é˜Ÿåˆ—ä¸­å¯èƒ½çš„ä¸‹ä¸€ä¸ªç‰‡æ®µ)
        do_rerun()
    else:
        # 2b. å¦‚æœæ²¡æœ‰æ–°ç‰‡æ®µ (VADé˜Ÿåˆ—ä¸ºç©º)ï¼ŒçŸ­æš‚ç¡çœ  (ä¾‹å¦‚ 100ms)
        #     ä»¥é¿å…CPUç©ºè½¬ï¼Œç„¶åå†é‡ç»˜ä»¥ç»§ç»­è½®è¯¢
        time.sleep(0.1)
        do_rerun()

# ======================= å¤‡ç”¨ï¼šæ–‡æœ¬è¾“å…¥å‘é€ =======================
with st.expander("âœï¸ ä¹Ÿå¯ç›´æ¥è¾“å…¥æ–‡æœ¬"):
    col_input, col_send = st.columns([8, 2])
    with col_input:
        text_input = st.text_input("è¾“å…¥æ–‡æœ¬ï¼ˆEnter å‘é€ï¼‰", key="text_input")
    with col_send:
        send_btn = st.button("å‘é€", use_container_width=True)

    if send_btn and text_input.strip():
        u = text_input.strip()
        _append_message("user", u)
        try:
            reply = medllm_chat(model, tokenizer, st.session_state["messages"])
        except Exception as e:
            reply = f"å¯¹è¯å¤±è´¥ï¼š{e}"
        audio_bytes = Text_to_audio(Text=reply)
        _append_message("assistant", reply, audio=audio_bytes)
        do_rerun()

# ======================= åº•éƒ¨å·¥å…·æ ï¼ˆå•åˆ—ï¼‰ =======================
st.divider()


def _clear_chat():
    st.session_state["messages"] = []
    st.session_state["asr_segments"] = []
    st.session_state["need_loop"] = False
    st.session_state["known_speakers"] = []
    st.session_state["last_emr_text"] = ""
    if 'ctx' in globals() and ctx and ctx.audio_processor:
        ctx.audio_processor.reset()


st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯", on_click=_clear_chat, use_container_width=True)
